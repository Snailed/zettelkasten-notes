Line Search Methods

Zoutendijk
Newton Descent Theorem

Line search: $x_{k+1} = x_k + \alpha_k p_k$
$\alpha$ = step size
$p$ = direction
Finding the right step size and the right direction is essential.

Descent direction can have a very specific definition based on Taylor's Theorem. Remember that the negative gradient points to the steepest descent.

Steepest descent requires very small steps
Newton's method is one of the fastest, but requires the hessian to always be positive definite.

If we take too big steps, we will never converge
If we take too small steps, we will end in the wrong critical point

Wolfe's conditions
The first one is Armijo's inequality. If you take a step, you expect to descent.
The second one is curvature condition. We expect the slope at the point of the next step to increase (e.g. we are closer to a minimum).

Non vacuity of Wolfe Conditions
Mean value theorem

Backtracking line search
start with a large candidate step size  and decrease it by a factor (0,1) as long as the first Wolfe condition is not satisfied.

Classical theorem