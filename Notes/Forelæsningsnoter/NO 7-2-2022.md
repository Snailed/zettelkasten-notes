[slides](file:///hdd/books/numerical-optimization/lectures/NO_Week1_handout.pdf)
Organization
	Implement optimization algorithm
	Evaluate correctness
	benchmarking
	understand ideas behind the algorithms
	follow theoretical derivations
	a lot of decisionmaking

Introduction
	We are given an objective function $f\colon \mathbb{R}^n \rightarrow \mathbb{R}$
	We want to find a point in a feasable region where $f$ is minimal.
	Our task is to develop an optimization that can find $x^*$. This requires math, programming and numerical handling
	
Linear Algebra refresher
	Vector norms (manhattan, euclidian, infinity norm). By default, euclidian.
	Orthogonal matrices
	Symmetric eigenvalue decomposition (sum form, matrix form) (used to solve problem 1)
	Matrix norms based on vector norms
	
Conditioning
	All algorithms make small errors. We always gets a solution $\tilde{x}=x+\epsilon$
	How sensitive is a function to small deviations
	This is called the _condition number_ (the worst case conditioning, long formula)
	$\kappa$
	At high $\kappa(A)$, small perputations can dominate function-value and gradient
	
Calculus
	Big-o, little-o, Omega. Often, we only care within a certain domain
	Taylor's theorem - very important
	Taylor's theorem in multiple dimensions - usually second order is enough - you just need a gradient and a Hessian
	Quadratic > superlinear > linear
	
	
	
	