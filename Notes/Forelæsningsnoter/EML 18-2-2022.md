Bootstrap methods
Bootstrapping like... doesn't work. But we assume it does.
The bootstrapping principle
Non-parametric bootstrap
Parametric bootstrap

Bootstrap estimates agree with maximum likelihood in most cases, but is most interesting when no analytical solution is possible.

Bagging - average prediction over bootstrap samples
Each bootstrap prediction can vote to do classification with K classes.
Reduces variability of individual base learners
Bagging does not help on linear regression. Bagging bad models leads to worse models.

Loss of Interpretability

You can do a weighted average of different models. Don't use the training data to evaluate the weight of each model though!


Boosting: If you have a weak model/classifier, then you can sequentially train them to an overall strong classifier, reweighting data such that future weak learners focus on currently wrongly classified data.

AdaBoost
	for m to M
		fit classifier
		fit weighted error
		compute constant
		set new weights for each misclassified point using contant

Gradient Boosting
Early Stop: If we stop making progress on the validation set, stop training.

Stochastic Gradient Boosting might help avoid local minima
