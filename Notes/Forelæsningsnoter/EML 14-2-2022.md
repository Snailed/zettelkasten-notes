Mixture models and EM

Mixture models: We want to be able to draw a sample (generative model)
We assume a causal relationship, and we want to model that. If this assumption is wrong, we will never get ggood model.
Gaussian probabilistic process.
Describe commplex distributions by convex combintations og simpler distributions

A mixture model produces a PDF based on multiple PDFs. The integral will still be 1!

A multivariate Gaussian distribution consists of a normalization, the mean and the presicion matrix

it is easy to sample from a gaussian distribution

EM algorithm for Gaussian Mixture models
Responsibility = $\gamma_{nk} \in [0,1]$.
Responsibility is the probability that a point was generated by one of the $k$ distributions in a gaussian mixture model.

We don't actually calculate the gradient, but instead do a two-step approach:
1. Calculate responsibilities
2. Given responsibilities, calculate $\pi_k$, means and covariance matrices

In the general case, we want to calculate the likelihood and the log-likelihood.
Kullback-Leibler divergence
	A measure of difference between distributions
	Always positive, zero if the distributions are the same
	Not symmetrical: $KL(p,q) \not = KL(q,p)$

Variational Lower Bound:
	Main statement?
	The log likelihood is the variational lower bound + the Kullback Leibler divergence

Lower bound property is derived from this.
Logarithmic evidence, evidence lower bound (ELBO).

E step: We try to find the optimal distribution over the hidden variables (this is mapped to $\pi$ in the GMM)
M step: From this assumption over the distribution over the hidden variables, try to calculate the arguments to our distribution of our observed variables.

1-hot encoding
