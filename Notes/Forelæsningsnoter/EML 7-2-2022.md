[slides](file:///hdd/books/eml/lectures/ml_concepts.pdf)
Supervised:
	Classification:
		- Output variables are categorical/factors $Y=\{y_1,\dots y_n\},2\leq n \infty$
	Regression
		- Output variables are ordered, quantitiative $Y\in \mathbb{R}^n, 1 \leq m < \infty$
		- Examples: K-means, Gaussian mixtures models (soft k-means)
	Density estimation
		- Output variables are probability distributions $Pr(y|x)$ on $Y$
	
Unsupervised
	Dimensionality Reduction (PCA, self-organizing maps, multidimensional scaling)
	Learning associations:
		- $Pr(Y|X)$
	Grouping



Learning machines:
- A hypothesis $h\colon \mathcal{X} \rightarrow \mathcal{Y}$ maps inputs to outputs, a hypothesis class $\mathcal{H}$ is a set of such functions.
- A machine learning algorithm maps from a sample of data to a hypothesis from $\mathcal{H}$
- The quality of the prediction of a hypothesis is quantified by a loss function ($L\colon Y \times Y \rightarrow [0, \infty[$) (example: 0-1 loss)
- We want to find $\arg \min_{h\in \mathcal{H}} \mathbb{E}(L(x, h))$ 
- Empirical risk is the average loss over the entire sample
- The risk of a hypothesis is the expected loss 
- Squared loss for regression
- Bayes risk is the minimum risk over all possible measurabel functions $h\in \mathcal{H}$. It is a tool to measure how well our learning algorithm is working. It is more to see if our algorithm is approaching our Bayes' risk. 
- An algorithm is consistent if the empirical risk approaches the bayes risk.

Overfitting
- An overfitted algorithm has a low empirical risk, but performs poorly on new data.
- Generalization error (test loss) - the error over an independent test sample (conditioned on our training set)
- Prediction error vs model complexity plot (classic, training sample and test sample diverges when model complexity increases. Test sample forms a valley)
- Bias: The difference between the avergage prediction and the correct value (due to wrong model assumptions)
- Varaince: The variability of predictions (due to paying too much attention to each training data point)
- Error can often be reduced to three things: Irreducible error + Bias^2 + Variance^2

Common to use three test splits:
Train (~50%) - fit the models
Validation (~25%) - estimate prediction error and select best model
Test (~25%) - access generalization error of the best model

Hyperparameters: Parameters not inferred on the data set.

To compensate for lack of data, we can try to estimate the in-sample prediction error (e.g. by adjusting for our overfitting)
 - Akaike information criterion
 - Bayesion information criterion
 - Minimum description length
 - Vapnik-Chervonenkis dimension
 
 We often just use Cross-validation (split data into $K$ folds, train on most, test and validate of some. Repeat $K-1$ times.)
 
 Parametric model: Summarizes data with parameters of fixed size
 - Constrained by their underlying functional form
 Non-parametric: Reverse
 
 Small neural networks are parametric, large are non-parametric
 
 Curse of dimensionality: Sweet spot in number of features (think computation power, overfitting)
 No free lunch theorem - no method is better than any other method
 
 Regularization: Modifications to reduce the generalization error but not its training error
 Explicit regularization vs implicit regularization