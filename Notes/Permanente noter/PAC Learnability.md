[[Shalev-Shwartz & Ben-David, 2014 - Understanding Machine Learning From Theory to Algorithms]]
[[Yehudayoff, 2023 - ATML Lecture Notes]]

There exists multiple definitions for this.

## The $\delta$-definition
A hypothesis class $\mathcal{H}$ is PAC learnable if there exists a function $m_\mathcal{H} \colon (0,1)^2 \rightarrow \mathbb{N}$ and a learning algorithm with the following property: For every $\epsilon, \delta \in (0,1)$, for every distribution $\mathcal{D}$ over $\mathcal{X}$, and for every labeling function $f \colon \mathcal{X} \rightarrow \{0, 1\}$, if the realizable assumption holds with respect to $\mathcal{H}$, $\mathcal{D}$, $f$, then when running the learning algorithm on $m \geq m_\mathcal{H}(\epsilon, \delta)$ i.i.d. examples generated by $\mathcal{D}$ and labeled by $f$, the algorithm returns a hypothesis $h$ such that, with probability of at least $1-\delta$ (over the choice of the examples), $$L_{(\mathcal{D}, f)}(h) \leq \epsilon$$

## The simple ATML definition
Let $\mu$ be the underlying, data-generating distribution and let $L_\mu$ be the [[Population loss]]. Let $H$ be the hypothesis class i.e. the space from which we pick $h$. Then, in the **realizable setting**, (i.e. $L_\mu(H) = 0$), $H$ is PAC-learnable if for every $\epsilon > 0$, there is an algorithm $A$ with a sample size $n$ such that for all realizable $\mu$
$$L_\mu(A) < \epsilon$$